{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c177f93-f39c-4139-b0dd-04703c23806e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "726ac468-aeca-468c-836e-c391dc196fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\envs\\basellmchatbot\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from transformers import BitsAndBytesConfig\n",
    "from langchain.llms import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1d7a29b-a51d-4039-a950-0bfad9defe41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\envs\\basellmchatbot\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Lenovo\\.cache\\huggingface\\hub\\models--microsoft--phi-2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading shards: 100%|██████████| 2/2 [03:34<00:00, 107.04s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:47<00:00, 23.91s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name: str = 'microsoft/phi-2'\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08ed6243-add6-4004-9cc7-5baa2d1e0b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hf_llm(model_name: str = \"microsoft/phi-2\", max_new_tokens = 1024, **kwargs):\n",
    "    \"\"\"\n",
    "    Creates and returns a HuggingFace LLM pipeline for text generation.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name of the pre-trained model to load from the HuggingFace model hub.\n",
    "                         Defaults to \"microsoft/phi-2\".\n",
    "        max_new_tokens (int): The maximum number of tokens to generate in the output sequence.\n",
    "                              Defaults to 1024.\n",
    "        **kwargs: Additional keyword arguments that can be passed to customize the model or tokenizer.\n",
    "\n",
    "    Returns:\n",
    "        llm: A HuggingFacePipeline instance configured for text generation.\n",
    "    \"\"\"\n",
    "    # Load a pre-trained causal language model with low memory usage optimization for CPUs.\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    # Load the tokenizer corresponding to the specified model.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    # Create a text-generation pipeline using the model and tokenizer.\n",
    "    model_pipeline = pipeline(\n",
    "        'text-generation',\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        device_map='cpu'\n",
    "    )\n",
    "    # Define additional generation parameters, such as sampling temperature.\n",
    "    gen_kwargs = {\n",
    "        'temperature': 0.5\n",
    "    }\n",
    "    # Wrap the pipeline in a HuggingFacePipeline object for further use.\n",
    "    llm = HuggingFacePipeline(\n",
    "        pipeline=model_pipeline,\n",
    "        model_kwargs=gen_kwargs\n",
    "    )\n",
    "\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057d7225-4ed7-4f8a-8171-b9a003d7f321",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
