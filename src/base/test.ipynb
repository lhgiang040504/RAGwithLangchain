{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c177f93-f39c-4139-b0dd-04703c23806e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "726ac468-aeca-468c-836e-c391dc196fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from transformers import BitsAndBytesConfig\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.llms import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a968f61-e48c-4ce0-9309-05575ab51190",
   "metadata": {},
   "source": [
    "# Call api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0708e6f-2271-4b44-9b33-af6553a3401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access token\n",
    "HF_TOKEN = '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'\n",
    "# create llm model\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"google/flan-t5-large\",  # Replace with the model of your choice\n",
    "    model_kwargs={\"temperature\": 0.5, \"max_length\": 256},\n",
    "    huggingfacehub_api_token=HF_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4072463a-7647-4022-8469-1a56ea4d46c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning is the process of learning to recognize patterns in data.\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 1.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Test the model with a prompt\n",
    "response = llm.invoke(\"Explain the concept of machine learning in simple terms.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a67d01a-140a-4589-be9a-28cdc9240c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access token\n",
    "HF_TOKEN = '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'\n",
    "# create llm model\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"microsoft/phi-2\",  # Replace with the model of your choice\n",
    "    model_kwargs={\"temperature\": 0.5, \"max_length\": 256},\n",
    "    huggingfacehub_api_token=HF_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb2952dc-ab49-4960-9a44-d4352845c5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain the concept of machine learning in simple terms.\n",
      "## INPUT\n",
      "\n",
      "##OUTPUT\n",
      "Machine learning is a way of teaching computers to learn from data and make predictions or decisions without being explicitly programmed. For example, a machine learning algorithm can learn from a large amount of pictures of cats and dogs and then be able to identify whether a new picture is of a cat or a dog. Machine learning can be used for various tasks, such as image recognition, natural language processing, recommendation systems, and self-driving cars. Machine learning is based on the idea that computers can learn from patterns and examples in data, and improve their performance over time.\n",
      "\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 795 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Test the model with a prompt\n",
    "response = llm.invoke(\"Explain the concept of machine learning in simple terms.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c23be56-33e1-4dd5-bfcd-9c514e8e4a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\envs\\basellmchatbot\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Lenovo\\.cache\\huggingface\\hub\\models--google--flan-t5-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 203 ms\n",
      "Wall time: 6.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load the tokenizer corresponding to the specified model.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61638692-32bb-4b6e-8e9c-2b920bd655a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 156 ms\n",
      "Wall time: 558 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load the tokenizer corresponding to the specified model.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655db1c2-4ee9-480d-8779-09455e49cd3c",
   "metadata": {},
   "source": [
    "# Self-hosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1d7a29b-a51d-4039-a950-0bfad9defe41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\envs\\basellmchatbot\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Lenovo\\.cache\\huggingface\\hub\\models--microsoft--phi-2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading shards: 100%|██████████| 2/2 [03:34<00:00, 107.04s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:47<00:00, 23.91s/it]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "model_name: str = 'microsoft/phi-2'\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76808f37-7a92-44d7-9167-8b16d8c22e40",
   "metadata": {},
   "source": [
    "# Connect function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "15574893-0003-4a12-a721-7736aa6382da",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3242769212.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[36], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    Additional Parameters and Considerations:\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Additional Parameters and Considerations:\n",
    "While these are the primary parameters used in the given code, there are other potential parameters you might consider depending on the specific model and task:\n",
    "\n",
    "top_p: Controls the diversity of the generated text by selecting the top-p most probable tokens.\n",
    "num_beams: Specifies the number of beams to use for beam search decoding, which can improve the quality of the generated text.\n",
    "early_stopping: Determines whether to stop the generation process early if the generated text reaches a certain quality threshold.\n",
    "no_repeat_ngram_size: Prevents the model from repeating the same ngram (sequence of tokens) multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "08ed6243-add6-4004-9cc7-5baa2d1e0b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hf_llm(model_name: str = \"microsoft/phi-2\", max_new_tokens = 1024, **kwargs):\n",
    "    \"\"\"\n",
    "    Creates and returns a HuggingFace LLM pipeline for text generation.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name of the pre-trained model to load from the HuggingFace model hub.\n",
    "                         Defaults to \"microsoft/phi-2\".\n",
    "        max_new_tokens (int): The maximum number of tokens to generate in the output sequence.\n",
    "                              Defaults to 1024.\n",
    "        **kwargs: Additional keyword arguments that can be passed to customize the model or tokenizer.\n",
    "\n",
    "    Returns:\n",
    "        llm: A HuggingFacePipeline instance configured for text generation.\n",
    "    \"\"\"\n",
    "    # Access token\n",
    "    HF_TOKEN = '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'\n",
    "    \n",
    "    # Define additional generation parameters, such as sampling temperature.\n",
    "    gen_kwargs = {\n",
    "        'temperature': kwargs.get('temperature', 0.5),\n",
    "        'max_new_tokens': kwargs.get('max_new_tokens', max_new_tokens)\n",
    "    }\n",
    "    \n",
    "    # create llm model\n",
    "    llm = HuggingFaceHub(\n",
    "        repo_id=model_name,\n",
    "        model_kwargs=gen_kwargs,\n",
    "        huggingfacehub_api_token=HF_TOKEN\n",
    "    )\n",
    "\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fc0cb4a0-d7cf-412f-bf0f-e2ae69ea1684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Using default parameters\n",
    "llm = get_hf_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0092e4da-c5cc-4c6d-a7f6-a75d28d2573b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain the concept of machine learning in simple terms.\n",
      "## INPUT\n",
      "\n",
      "##OUTPUT\n",
      "Machine learning is a branch of artificial intelligence that allows computers to learn from data and make predictions or decisions without being explicitly programmed. For example, a machine learning algorithm can learn to recognize faces from a large dataset of images and then identify new faces in new images. Machine learning can be used for various tasks, such as image analysis, speech recognition, natural language processing, and recommendation systems.\n",
      "\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 1min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Test the model with a prompt\n",
    "response = llm.invoke(\"Explain the concept of machine learning in simple terms.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3ac79c57-5f5c-4d5a-b8d5-247f7f636dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Customizing model and temperature\n",
    "llm = get_hf_llm(temperature=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8f1b7530-3973-4f2f-b73a-496b240440d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain the concept of machine learning in simple terms. No input. OUTPUT: Machine learning is a type of artificial intelligence that allows computers to learn from data and experience, without being explicitly programmed. Machine learning algorithms can analyze large amounts of data, find patterns and trends, and make predictions or decisions based on the data. Machine learning can be used for various applications, such as image recognition, natural language processing, recommendation systems, and self-driving cars.\n",
      "\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 1.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Test the model with a prompt\n",
    "response = llm.invoke(\"Explain the concept of machine learning in simple terms.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f1c8e7f2-548a-4f69-9530-53df8e29f0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Customizing max_new_tokens and temperature\n",
    "llm = get_hf_llm(max_new_tokens=512, temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bf3fe9d7-b3ce-4137-b3f4-3eec1aed39d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain the concept of machine learning in simple terms.\n",
      "## INPUT\n",
      "Machine learning is a branch of artificial intelligence that enables computers to learn from data and experience without being explicitly programmed.\n",
      "##OUTPUT\n",
      "Machine learning is a way of making computers smarter by letting them learn from what they see and do.\n",
      "\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 12.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Test the model with a prompt\n",
    "response = llm.invoke(\"Explain the concept of machine learning in simple terms.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b259483-cc56-4a72-be67-cf2899fc9a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Customizing all parameters\n",
    "llm = get_hf_llm(max_new_tokens=256, temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "057d7225-4ed7-4f8a-8171-b9a003d7f321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain the concept of machine learning in simple terms.\n",
      "## INPUT\n",
      "Machine learning is a branch of artificial intelligence that enables computers to learn from data and experience without being explicitly programmed.\n",
      "##OUTPUT\n",
      "Machine learning is a way of making computers smarter by letting them learn from what they see and do.\n",
      "\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 387 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Test the model with a prompt\n",
    "response = llm.invoke(\"Explain the concept of machine learning in simple terms.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a6354e-9a2b-4862-927f-9b3089f0ba5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
