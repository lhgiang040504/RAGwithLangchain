{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9244f3b-5cac-4c7c-ae30-ce9d9862e6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fbde83d-3f9e-4a3a-9444-e3dd1f4634bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98ed89a5-7a7e-414e-98b2-cbaf97daa39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_API_KEY\"] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38693afd-4e11-4afa-a20b-0caff632fe1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\anaconda3\\envs\\basellmchatbot\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Lenovo\\learning\\RAGwithLangchain\\src\\rag\\vectorstore.py:12: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding: HuggingFaceEmbeddings = HuggingFaceEmbeddings(),\n",
      "c:\\Users\\Lenovo\\learning\\RAGwithLangchain\\src\\rag\\vectorstore.py:12: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embedding: HuggingFaceEmbeddings = HuggingFaceEmbeddings(),\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "\n",
    "from langserve import add_routes\n",
    "\n",
    "from base.llm_model import get_hf_llm\n",
    "from rag.main import build_rag_chain, InputQA, OutputQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d00dd7b-1520-464d-b15e-180364875772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Lenovo\\\\learning\\\\RAGwithLangchain\\\\src'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f040d9c-8214-452d-8f62-9af0b2bc9512",
   "metadata": {},
   "source": [
    "# SELF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3da6aa72-c85b-413a-9d9d-bb2705f90070",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\learning\\RAGwithLangchain\\src\\base\\llm_model.py:34: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
      "  llm = HuggingFaceHub(\n"
     ]
    }
   ],
   "source": [
    "llm = get_hf_llm(model_name='mistralai/Mistral-7B-Instruct-v0.2', temperature=0.9)\n",
    "genai_docs = \"../data_source/generative_ai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ca22198-ef03-4a5b-bdf0-b1eb49c178c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- max worker = 16 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading PDFs: 100%|██████████| 6/6 [02:12<00:00, 22.16s/file]\n"
     ]
    }
   ],
   "source": [
    "genai_chain = build_rag_chain(llm, data_dir=genai_docs, data_type='pdf', workers_for_load=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df3c0024-4d2f-44af-9ed1-4764b2911e0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_data = InputQA(question=\"What is Attention?\")\n",
    "output_data = genai_chain.invoke(input_data.question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cb7fcfe-9340-4e76-a477-4b5343a00d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Attention is a function described in the given text as a way to compute a weighted sum of vectors, based on query, keys, and values. It's used in models like the Transformer for understanding relationships between different parts of data, such as text. Self-attention, a type of attention, helps in understanding relationships within the same data.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ab6c6a-85e5-4adf-aa17-c816559b1195",
   "metadata": {},
   "source": [
    "# Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1820e33b-4ff1-40e3-94c4-ead4ce758e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# App - FastAPI\n",
    "app = FastAPI(\n",
    "    title=\"LangChain Server\",\n",
    "    version=\"1.0\",\n",
    "    description=\"A simple api server using LangChain's Runnable interfaces\"\n",
    ")\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    "    expose_headers=[\"*\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1baf648-f749-4568-9995-d3655714ffa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routes - FastAPI\n",
    "@app.get(\"/check\")\n",
    "async def check():\n",
    "    return {\"status\": \"ok\"}\n",
    "\n",
    "@app.post(\"/generative_ai\", response_model=OutputQA)\n",
    "async def generative_ai(inputs: InputQA):\n",
    "    answer = genai_chain.invoke(inputs.question)\n",
    "    return {\"answer\": answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b15be54-e1ee-4094-b3fe-ba680e15a5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langserve Routes - Playground\n",
    "add_routes(app,\n",
    "    genai_chain,\n",
    "    playground_type=\"default\",\n",
    "    path=\"/generative_ai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4db34aca-cbf4-422b-89f4-6dd8a9421038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "--- max worker = 16 ---\n",
      "\n",
      "     __          ___      .__   __.   _______      _______. _______ .______     ____    ____  _______\n",
      "    |  |        /   \\     |  \\ |  |  /  _____|    /       ||   ____||   _  \\    \\   \\  /   / |   ____|\n",
      "    |  |       /  ^  \\    |   \\|  | |  |  __     |   (----`|  |__   |  |_)  |    \\   \\/   /  |  |__\n",
      "    |  |      /  /_\\  \\   |  . `  | |  | |_ |     \\   \\    |   __|  |      /      \\      /   |   __|\n",
      "    |  `----./  _____  \\  |  |\\   | |  |__| | .----)   |   |  |____ |  |\\  \\----.  \\    /    |  |____\n",
      "    |_______/__/     \\__\\ |__| \\__|  \\______| |_______/    |_______|| _| `._____|   \\__/     |_______|\n",
      "    \n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m Playground for chain \"/generative_ai/\" is live at:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\envs\\basellmchatbot\\lib\\site-packages\\langchain\\llms\\__init__.py:549: LangChainDeprecationWarning: Importing LLMs from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n",
      "\n",
      "`from langchain_community.llms import HuggingFacePipeline`.\n",
      "\n",
      "To install langchain-community run `pip install -U langchain-community`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\envs\\basellmchatbot\\lib\\site-packages\\langchain\\llms\\__init__.py:549: LangChainDeprecationWarning: Importing LLMs from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n",
      "\n",
      "`from langchain_community.llms import HuggingFaceHub`.\n",
      "\n",
      "To install langchain-community run `pip install -U langchain-community`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\envs\\basellmchatbot\\lib\\site-packages\\langchain\\llms\\__init__.py:549: LangChainDeprecationWarning: Importing LLMs from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n",
      "\n",
      "`from langchain_community.llms import HuggingFaceHub`.\n",
      "\n",
      "To install langchain-community run `pip install -U langchain-community`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\learning\\RAGwithLangchain\\src\\rag\\vectorstore.py:12: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding: HuggingFaceEmbeddings = HuggingFaceEmbeddings(),\n",
      "C:\\Users\\Lenovo\\learning\\RAGwithLangchain\\src\\rag\\vectorstore.py:12: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embedding: HuggingFaceEmbeddings = HuggingFaceEmbeddings(),\n",
      "C:\\Users\\Lenovo\\learning\\RAGwithLangchain\\src\\base\\llm_model.py:34: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
      "  llm = HuggingFaceHub(\n",
      "\n",
      "Loading PDFs:   0%|          | 0/6 [00:00<?, ?file/s]\n",
      "Loading PDFs:  17%|#6        | 1/6 [00:10<00:53, 10.74s/file]\n",
      "Loading PDFs:  33%|###3      | 2/6 [00:14<00:27,  6.90s/file]\n",
      "Loading PDFs:  50%|#####     | 3/6 [00:32<00:34, 11.54s/file]\n",
      "Loading PDFs:  67%|######6   | 4/6 [01:41<01:08, 34.28s/file]\n",
      "Loading PDFs:  83%|########3 | 5/6 [01:57<00:27, 27.69s/file]\n",
      "Loading PDFs: 100%|##########| 6/6 [02:12<00:00, 23.43s/file]\n",
      "Loading PDFs: 100%|##########| 6/6 [02:12<00:00, 22.05s/file]\n",
      "INFO:     Started server process [22336]\n",
      "INFO:     Waiting for application startup.\n",
      "ERROR:    Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\envs\\basellmchatbot\\lib\\site-packages\\starlette\\routing.py\", line 693, in lifespan\n",
      "    async with self.lifespan_context(app) as maybe_state:\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\envs\\basellmchatbot\\lib\\site-packages\\starlette\\routing.py\", line 569, in __aenter__\n",
      "    await self._router.startup()\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\envs\\basellmchatbot\\lib\\site-packages\\starlette\\routing.py\", line 670, in startup\n",
      "    await handler()\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\envs\\basellmchatbot\\lib\\site-packages\\langserve\\server.py\", line 239, in startup_event\n",
      "    print(f'{green(\"LANGSERVE:\")}  \\u2502')\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\envs\\basellmchatbot\\lib\\encodings\\cp1252.py\", line 19, in encode\n",
      "    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n",
      "UnicodeEncodeError: 'charmap' codec can't encode character '\\u2502' in position 26: character maps to <undefined>\n",
      "\n",
      "ERROR:    Application startup failed. Exiting.\n"
     ]
    }
   ],
   "source": [
    "!uvicorn app:app --host \"0.0.0.0\" --port 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c80e92d-9d1a-4980-b87b-29dfc0a7bd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!uvicorn test:app --host \"127.0.0.1\" --port 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bd5024f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\anaconda3\\envs\\basellmchatbot\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "(Request ID: As6eUF_98QSnPvOpdtkfM)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\envs\\basellmchatbot\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\envs\\basellmchatbot\\lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: https://api-inference.huggingface.co/models/meta-llama/Llama-3.3-70B-Instruct/v1/chat/completions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 12\u001b[0m\n\u001b[0;32m      3\u001b[0m client \u001b[38;5;241m=\u001b[39m InferenceClient(api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_gQvRaioNtlYrvOzGLvhrAyxbcJZZfpqVfE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      6\u001b[0m \t{\n\u001b[0;32m      7\u001b[0m \t\t\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m \t\t\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the capital of France?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m \t}\n\u001b[0;32m     10\u001b[0m ]\n\u001b[1;32m---> 12\u001b[0m completion \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta-llama/Llama-3.3-70B-Instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\n\u001b[0;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage)\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\envs\\basellmchatbot\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:882\u001b[0m, in \u001b[0;36mInferenceClient.chat_completion\u001b[1;34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p)\u001b[0m\n\u001b[0;32m    860\u001b[0m payload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m    861\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel_id,\n\u001b[0;32m    862\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    879\u001b[0m     stream_options\u001b[38;5;241m=\u001b[39mstream_options,\n\u001b[0;32m    880\u001b[0m )\n\u001b[0;32m    881\u001b[0m payload \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m payload\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[1;32m--> 882\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _stream_chat_completion_response(data)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\envs\\basellmchatbot\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:296\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[1;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 296\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\envs\\basellmchatbot\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:460\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n\u001b[0;32m    457\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBad request for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m endpoint:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m endpoint_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBad request:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m     )\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(BadRequestError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[0;32m    463\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    464\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Forbidden: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    465\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCannot access content at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    466\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure your token has the correct permissions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    467\u001b[0m     )\n",
      "\u001b[1;31mBadRequestError\u001b[0m: (Request ID: As6eUF_98QSnPvOpdtkfM)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query."
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(api_key=\"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\")\n",
    "\n",
    "messages = [\n",
    "\t{\n",
    "\t\t\"role\": \"user\",\n",
    "\t\t\"content\": \"What is the capital of France?\"\n",
    "\t}\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3.3-70B-Instruct\", \n",
    "\tmessages=messages, \n",
    "\tmax_tokens=500\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286a8d81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basellmchatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
